name: CD Pipeline

on:
  push:
    branches: [ main ]
    tags: [ 'v*' ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'staging'
        type: choice
        options:
        - staging
        - production

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build-and-push:
    name: Build and Push Images
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    
    outputs:
      version: ${{ steps.meta.outputs.version }}
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha
    
    - name: Build and push Backend
      uses: docker/build-push-action@v5
      with:
        context: ./src/backend
        file: ./src/backend/Dockerfile.production
        platforms: linux/amd64,linux/arm64
        push: true
        tags: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/backend:latest
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/backend:${{ steps.meta.outputs.version }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
    
    - name: Build and push Frontend
      uses: docker/build-push-action@v5
      with:
        context: ./frontend
        file: ./frontend/Dockerfile.production
        platforms: linux/amd64,linux/arm64
        push: true
        tags: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/frontend:latest
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/frontend:${{ steps.meta.outputs.version }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: build-and-push
    if: github.ref == 'refs/heads/develop' || (github.event_name == 'workflow_dispatch' && github.event.inputs.environment == 'staging')
    environment:
      name: staging
      url: https://staging.tradesense.com
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'
    
    - name: Configure kubectl
      run: |
        echo "${{ secrets.STAGING_KUBECONFIG }}" | base64 --decode > kubeconfig
        export KUBECONFIG=kubeconfig
    
    - name: Update image tags
      run: |
        cd k8s
        sed -i "s|tradesense/backend:latest|${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/backend:${{ needs.build-and-push.outputs.version }}|g" backend.yaml
        sed -i "s|tradesense/frontend:latest|${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/frontend:${{ needs.build-and-push.outputs.version }}|g" frontend.yaml
    
    - name: Deploy to staging
      run: |
        export KUBECONFIG=kubeconfig
        cd k8s
        kubectl apply -f namespace.yaml
        kubectl apply -f configmap-staging.yaml
        kubectl apply -f secrets-staging.yaml
        kubectl apply -f postgres.yaml
        kubectl apply -f redis.yaml
        kubectl apply -f backend.yaml
        kubectl apply -f frontend.yaml
        kubectl apply -f ingress-staging.yaml
    
    - name: Wait for deployment
      run: |
        export KUBECONFIG=kubeconfig
        kubectl wait --for=condition=available --timeout=300s deployment/backend -n tradesense
        kubectl wait --for=condition=available --timeout=300s deployment/frontend -n tradesense
    
    - name: Run smoke tests
      run: |
        sleep 30
        curl -f https://staging.tradesense.com/api/v1/monitoring/health || exit 1
        curl -f https://staging.tradesense.com/health || exit 1

  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: build-and-push
    if: startsWith(github.ref, 'refs/tags/v') || (github.event_name == 'workflow_dispatch' && github.event.inputs.environment == 'production')
    environment:
      name: production
      url: https://tradesense.com
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'
    
    - name: Configure kubectl
      run: |
        echo "${{ secrets.PRODUCTION_KUBECONFIG }}" | base64 --decode > kubeconfig
        export KUBECONFIG=kubeconfig
    
    - name: Backup database
      run: |
        export KUBECONFIG=kubeconfig
        kubectl exec -n tradesense deployment/postgres -- pg_dump -U ${{ secrets.POSTGRES_USER }} tradesense > backup-$(date +%Y%m%d-%H%M%S).sql
    
    - name: Update image tags
      run: |
        cd k8s
        sed -i "s|tradesense/backend:latest|${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/backend:${{ needs.build-and-push.outputs.version }}|g" backend.yaml
        sed -i "s|tradesense/frontend:latest|${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/frontend:${{ needs.build-and-push.outputs.version }}|g" frontend.yaml
    
    - name: Deploy to production
      run: |
        export KUBECONFIG=kubeconfig
        cd k8s
        
        # Apply configurations
        kubectl apply -f namespace.yaml
        kubectl apply -f configmap.yaml
        kubectl apply -f secrets.yaml
        kubectl apply -f cert-manager.yaml
        
        # Deploy databases (skip if already exist)
        kubectl apply -f postgres.yaml
        kubectl apply -f redis.yaml
        
        # Rolling update for zero downtime
        kubectl apply -f backend.yaml
        kubectl rollout status deployment/backend -n tradesense
        
        kubectl apply -f frontend.yaml
        kubectl rollout status deployment/frontend -n tradesense
        
        # Update ingress
        kubectl apply -f ingress.yaml
    
    - name: Run health checks
      run: |
        sleep 60
        curl -f https://tradesense.com/api/v1/monitoring/health || exit 1
        curl -f https://tradesense.com/health || exit 1
    
    - name: Create release notes
      if: startsWith(github.ref, 'refs/tags/v')
      uses: actions/create-release@v1
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        tag_name: ${{ github.ref }}
        release_name: Release ${{ github.ref }}
        body: |
          ## Changes in this Release
          
          ### Backend
          - Docker image: `${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/backend:${{ needs.build-and-push.outputs.version }}`
          
          ### Frontend
          - Docker image: `${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/frontend:${{ needs.build-and-push.outputs.version }}`
          
          ### Deployment
          - Deployed to production cluster
          - Database backup created
          - Zero-downtime deployment completed
        draft: false
        prerelease: false

  rollback:
    name: Rollback Production
    runs-on: ubuntu-latest
    if: failure() && needs.deploy-production.result == 'failure'
    needs: [deploy-production]
    environment:
      name: production
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Configure kubectl
      run: |
        echo "${{ secrets.PRODUCTION_KUBECONFIG }}" | base64 --decode > kubeconfig
        export KUBECONFIG=kubeconfig
    
    - name: Rollback deployments
      run: |
        export KUBECONFIG=kubeconfig
        kubectl rollout undo deployment/backend -n tradesense
        kubectl rollout undo deployment/frontend -n tradesense
        kubectl rollout status deployment/backend -n tradesense
        kubectl rollout status deployment/frontend -n tradesense
    
    - name: Notify rollback
      uses: actions/github-script@v7
      with:
        script: |
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: '⚠️ Production deployment failed and was automatically rolled back!'
          })